{"cells":[{"metadata":{},"cell_type":"markdown","source":"The classification goal is to predict whether the patient has 10-year risk of future coronary heart disease (CHD)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport statsmodels.api as sm\nfrom statsmodels.tools import add_constant as add_constant\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom imblearn.over_sampling import SMOTE\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/framingham-heart-study-dataset/framingham.csv\")\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Columns/Variables Explained:\n* **sex**: male or female (male=1)\n* **age**: age of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)\n* **education**: 1 = Some High School; 2 = High School or GED; 3 = Some College or Vocational School; 4 = college\n* **currentSmoker**: whether or not the patient is a current smoker (current smoker=1)\n* **cigsPerDay**: the number of cigarettes that the person smoked on average in one day.\n* **BPMeds**: whether or not the patient was on blood pressure medication (on BP medication = 1)\n* **prevalentStroke**: whether or not the patient had previously had a stroke (previous stroke = 1)\n* **prevalentHyp**: whether or not the patient was hypertensive (hypertensive = 1)\n* **diabetes**: whether or not the patient had diabetes (diabetes = 1)\n* **totChol**: total cholesterol level.\n* **sysBP**: systolic blood pressure.\n* **diaBP**: diastolic blood pressure.\n* **BMI**: Body Mass Index.\n* **heartRate**: heart rate.\n* **glucose**: glucose level\n* **TenYearCHD**: 10 year risk of coronary heart disease CHD (yes=1)"},{"metadata":{},"cell_type":"markdown","source":"Find out percentage of nulls"},{"metadata":{"trusted":true},"cell_type":"code","source":"100* df.isnull().sum()/df.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the percentage of nulls is very less, let's drop those rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Percentage of People with heart disease: {0:.2f} %\".format(100*df.TenYearCHD.value_counts()[1]/df.TenYearCHD.count()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to be an Imbalance in data with respect to TenYearCHD. Only 15.23% rows have Positive value. We will first go ahead with the imbalance and see if it affects our analysis."},{"metadata":{},"cell_type":"markdown","source":"Let's create dummy variables for education."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df, pd.get_dummies(df.education, prefix=\"ed_\",drop_first=True)],axis=1)\ndf.drop(['education'], axis=1, inplace=True)\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use statsmodels' Logit funtion to perform logistic regression so that we get a clear picture of p-values , so that we find out statistically significant variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['TenYearCHD'], axis=1)\nY = df.TenYearCHD\n\nX_const=sm.add_constant(X)\nmodel=sm.Logit(Y, X_const)\nresult=model.fit()\nresult.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are lot of columns which are statistically insignificant. (p-value>0.05). We remove these columns in the next step"},{"metadata":{"trusted":true},"cell_type":"code","source":"def back_feature_elim(data_frame,dep_var,col_list):\n    \"\"\" Takes in the dataframe, the dependent variable and a list of column names, runs the regression repeatedly eliminating\n    feature with the highest p-value above alpha(0.05) one at a time and returns the regression summary with all \n    p-values below alpha\"\"\"\n\n    while len(col_list)>0 :\n        model=sm.Logit(dep_var,data_frame[col_list])\n        result=model.fit(disp=0)\n        largest_pvalue=round(result.pvalues,3).nlargest(1)\n        if largest_pvalue[0]<(0.05):\n            return result\n            break\n        else:\n            col_list=col_list.drop(largest_pvalue.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's remove statistically insignificant columns and run logistic regression again"},{"metadata":{"trusted":true},"cell_type":"code","source":"result=back_feature_elim(df,df.TenYearCHD, X.columns)\nresult.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = np.exp(result.params)\nconf = np.exp(result.conf_int())\nconf['OR'] = params\npvalue=round(result.pvalues,3)\nconf['pvalue']=pvalue\nconf.columns = ['CI 95%(2.5%)', 'CI 95%(97.5%)', 'Odds Ratio','pvalue']\nprint ((conf))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inferences from the above table<br>\n* Odds of getting diagnosed with heart disease for males is higher by 49.69% than females\n* Odds of getting diagnosed with heart disease increases by 2.83% every year.\n* Odds of getting diagnosed with heart disease increases by 1.42% with every additional cigarette\nand so on..."},{"metadata":{},"cell_type":"markdown","source":"Now that we have the dataset all cleaned up, Let's split the dataset with statistically significant columns in the ratio train:test=70:30"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df=df[['male','age','cigsPerDay','prevalentHyp','diabetes','sysBP','diaBP','BMI','heartRate','ed__2.0','ed__3.0','ed__4.0',\n           'TenYearCHD']]\nX = new_df.drop(['TenYearCHD'], axis=1)\nY = new_df.TenYearCHD\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg=LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred=logreg.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sklearn.metrics.accuracy_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test,y_pred)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nThe confusion matrix shows 937+8 = 945 correct predictions and 147+6= 153 incorrect ones.\n\nTrue Positives: 8\n\nTrue Negatives: 937\n\nFalse Positives: 6 (Type I error)\n\nFalse Negatives: 147 ( Type II error)"},{"metadata":{},"cell_type":"markdown","source":"**Since we are dealing with Predicting heart disease, Higher value of false negatives is dangerous. <br>\nFalse Negative = There is Heart Disease, but we predict it wrongly as No Heart Disease. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"TN=cm[0,0]\nTP=cm[1,1]\nFN=cm[1,0]\nFP=cm[0,1]\nsensitivity=TP/float(TP+FN)\nspecificity=TN/float(TN+FP)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The accuracy of the model [TP+TN/(TP+TN+FP+FN)] = ',(TP+TN)/float(TP+TN+FP+FN),'\\n',\n\n'Missclassification [1-Accuracy] = ',1-((TP+TN)/float(TP+TN+FP+FN)),'\\n',\n\n'Sensitivity or True Positive Rate [TP/(TP+FN)] = ',TP/float(TP+FN),'\\n',\n\n'Specificity or True Negative Rate [TN/(TN+FP)] = ',TN/float(TN+FP),'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, True Negatives rate is very high (since, the dataset had around 85% No Heart Disease). Because of this, our model was not able to learn much of Positives"},{"metadata":{"trusted":true},"cell_type":"code","source":"smt = SMOTE()\nx_train, y_train = smt.fit_sample(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.bincount(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg.fit(x_train,y_train)\ny_pred=logreg.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sklearn.metrics.accuracy_score(y_test,y_pred))\ncm = confusion_matrix(y_test,y_pred)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThe confusion matrix shows 621+104 = 805 correct predictions and 322+51= 373 incorrect ones.\n\nTrue Positives: 104\n\nTrue Negatives: 621\n\nFalse Positives: 322 (Type I error)\n\nFalse Negatives: 51 ( Type II error)"},{"metadata":{"trusted":true},"cell_type":"code","source":"TN=cm[0,0]\nTP=cm[1,1]\nFN=cm[1,0]\nFP=cm[0,1]\nsensitivity=TP/float(TP+FN)\nspecificity=TN/float(TN+FP)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The accuracy of the model [TP+TN/(TP+TN+FP+FN)] = ',(TP+TN)/float(TP+TN+FP+FN),'\\n',\n\n'Missclassification [1-Accuracy] = ',1-((TP+TN)/float(TP+TN+FP+FN)),'\\n',\n\n'Sensitivity or True Positive Rate [TP/(TP+FN)] = ',TP/float(TP+FN),'\\n',\n\n'Specificity or True Negative Rate [TN/(TN+FP)] = ',TN/float(TN+FP),'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, True positive rate has increased since we have balanced the data now(increasing Positive samples). Also, we were worried about False Negatives. This has decreased from 147 to 51 which is a good sign. Now our model will have lesser Type II errors."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}